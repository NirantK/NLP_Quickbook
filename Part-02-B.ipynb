{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python 3.6.3 :: Anaconda custom (64-bit)\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "__author__ = \"nirant.bits@gmail.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell Correction\n",
    "One of the most frequently seen text challenges is correcting spellings. This is even all the more true when data is entered by casual human users, for instance, say shipping addresses or similar. \n",
    "\n",
    "Let's take an example, we want to correct `Gujrat`, `Gujart` and other minor misspelligns to  `Gujarat`.\n",
    "There are several good ways to do this, depending on your dataset, and level of expertise. We discuss 2-3 popular ways, and discuss their pros and cons. \n",
    "\n",
    "Before I begin, we need to pay our homage to the legendary [Peter Norvig's Spell Correct](https://norvig.com/spell-correct.html). It's stil worth a read on how to _think_ about solving a problem and _exploring_ implementations. Even the way he refactors his code and writes functions is educational. \n",
    "\n",
    "His spell correction module is not the simplest or best way. I recommend two packages: one with a bias towards simplicity, one with a bias towards giving you all the knives, bells and whistles to try:\n",
    "\n",
    "**[FuzzyWuzzy](https://github.com/seatgeek/fuzzywuzzy)** is easy to use. It gives a simple similarity score between two strings, capped to 100. Higher numbers mean the words are more similar. \n",
    "\n",
    "**[Jellyfish](https://github.com/jamesturk/jellyfish)** supports 6 edit distance functions and 4 phonetic encoding options which you can use as per your use case. \n",
    "\n",
    "## FuzzyWuzzy\n",
    "\n",
    "If you have heard or used the [Levenshtein distance](https://www.wikiwand.com/en/Levenshtein_distance) (or, _edit distance_ functions in general), this package is a wrapper over the same. It uses difflib from the standard Python libs as well. \n",
    "\n",
    "If you want to consider implementing your own heurestics, I recommend checking out this insightful [StackOverflow Answer on Closest String Match](https://stackoverflow.com/questions/5859561/getting-the-closest-string-match)\n",
    "\n",
    "_Tip via Wiki: Edit distance is a way of quantifying how dissimilar two strings (e.g., words) are to one another by counting the minimum number of operations required to transform one string into the other._\n",
    "\n",
    "Let's see how we can use FuzzyWuzzy to correct our misspellings:\n",
    "\n",
    "**Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install fuzzywuzzy\n",
    "# alternative for 4-10x faster computation: \n",
    "# !{sys.executable} -m pip install fuzzywuzzy[speedup]\n",
    "\n",
    "# Collecting fuzzywuzzy\n",
    "#   Downloading fuzzywuzzy-0.16.0-py2.py3-none-any.whl\n",
    "# Installing collected packages: fuzzywuzzy\n",
    "# Successfully installed fuzzywuzzy-0.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"Electronic City Phase One\", \"Electronic City Phase One, Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_ratio(\"Electronic City Phase One\", \"Electronic City Phase One, Bangalore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the `ratio` function is confused by the trailing “Bangalore” used in address above, but really the two strings refer to the same address/entity. This is captured by `partial_ratio`. \n",
    "\n",
    "Note that how both `ratio` and `partial_ratio` are sensitive to ordering of the words. This is useful for comparing addresses, which follow a rough logical order. On the other hand, if we want to compare something else like person names, it might give counterintuitive results: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('Narendra Modi', 'Narendra D. Modi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_ratio('Narendra Modi', 'Narendra D. Modi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrgh, this is not nice. Just because we had an extra `D.` token, our logic is less universal. We want something that is less order sensitive. Luckily, the authors of fuzzywuzzy kept this in mind. \n",
    "\n",
    "They support functions which tokenize our input on space, remove punctuations, numbers and non-ASCII characters. Then this is used to calculate similarity. Let's try that out: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.token_sort_ratio('Narendra Modi', 'Narendra D. Modi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.token_set_ratio('Narendra Modi', 'Narendra D. Modi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Nice, this works perfectly for us. In case we have a list of options and we want to find the closest match(es), we can use the process module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Gujarat', 92), ('Gujarat Govt.', 75), ('Gujjar', 67)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Gujarat', 92)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Gujrat'\n",
    "choices = ['Gujarat', 'Gujjar', 'Gujarat Govt.']\n",
    "# Get a list of matches ordered by score, default limit to 5\n",
    "print(process.extract(query, choices))\n",
    "\n",
    "# If we want only the top one\n",
    "process.extractOne(query, choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Bangalore', 94), ('Bengaluru', 59)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Bangalore', 94)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Banglore'\n",
    "choices = ['Bangalore', 'Bengaluru']\n",
    "print(process.extract(query, choices))\n",
    "process.extractOne(query, choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chilli', 91), ('chilling', 77), ('chilled', 67)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('chilli', 91)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take an example of a common search typo in online shopping:\n",
    "query = 'chili'\n",
    "choices = ['chilli', 'chilled', 'chilling']\n",
    "print(process.extract(query, choices))\n",
    "process.extractOne(query, choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jellyfish\n",
    "\n",
    "This supports reasonably fast implementations of almost all popular edit distance functions (Remember? Edit distance functions tell how similar are two sequences/strings). While fuzzywuzzy supported mainly levenshtein distance, this package supports some more string comparison utilities:\n",
    "\n",
    "- Levenshtein Distance\n",
    "- Damerau-Levenshtein Distance\n",
    "- Jaro Distance\n",
    "- Jaro-Winkler Distance\n",
    "- Match Rating Approach Comparison\n",
    "- Hamming Distance\n",
    "\n",
    "Additionally, it supports **phonetic encodings** for English. \n",
    "\n",
    "**Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install jellyfish\n",
    "\n",
    "# # Collecting jellyfish\n",
    "# #   Downloading jellyfish-0.6.0-py3-none-any.whl\n",
    "# # Installing collected packages: jellyfish\n",
    "# # Successfully installed jellyfish-0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jellyfish\n",
    "correct_example = ('Narendra Modi', 'Narendra Modi')\n",
    "damodardas_example = ('Narendra Modi', 'Narendra D. Modi')\n",
    "modi_typo_example = ('Narendra Modi', 'Narendar Modi')\n",
    "gujarat_typo_example = ('Gujarat', 'Gujrat')\n",
    "\n",
    "examples = [correct_example, damodardas_example, modi_typo_example, gujarat_typo_example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(function, examples=examples):\n",
    "    for ele in examples:\n",
    "        print(f'{ele}: {function(*ele)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Narendra Modi', 'Narendra Modi'): 0\n",
      "('Narendra Modi', 'Narendra D. Modi'): 3\n",
      "('Narendra Modi', 'Narendar Modi'): 2\n",
      "('Gujarat', 'Gujrat'): 1\n"
     ]
    }
   ],
   "source": [
    "calculate_distance(jellyfish.levenshtein_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Narendra Modi', 'Narendra Modi'): 0\n",
      "('Narendra Modi', 'Narendra D. Modi'): 3\n",
      "('Narendra Modi', 'Narendar Modi'): 1\n",
      "('Gujarat', 'Gujrat'): 1\n"
     ]
    }
   ],
   "source": [
    "calculate_distance(jellyfish.damerau_levenshtein_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Narendra Modi', 'Narendra Modi'): 0\n",
      "('Narendra Modi', 'Narendra D. Modi'): 7\n",
      "('Narendra Modi', 'Narendar Modi'): 2\n",
      "('Gujarat', 'Gujrat'): 4\n"
     ]
    }
   ],
   "source": [
    "calculate_distance(jellyfish.hamming_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jaro and Jaro-Winkler** return a value of similarity - and not dissimilarity. This means that the perfect match returns 1.0 and totally unrelated match would tend to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Narendra Modi', 'Narendra Modi'): 1.0\n",
      "('Narendra Modi', 'Narendra D. Modi'): 0.9375\n",
      "('Narendra Modi', 'Narendar Modi'): 0.9743589743589745\n",
      "('Gujarat', 'Gujrat'): 0.8968253968253969\n"
     ]
    }
   ],
   "source": [
    "calculate_distance(jellyfish.jaro_distance) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Narendra Modi', 'Narendra Modi'): 1.0\n",
      "('Narendra Modi', 'Narendra D. Modi'): 0.9625\n",
      "('Narendra Modi', 'Narendar Modi'): 0.9846153846153847\n",
      "('Gujarat', 'Gujrat'): 0.9277777777777778\n"
     ]
    }
   ],
   "source": [
    "calculate_distance(jellyfish.jaro_winkler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Phonetic Word Similarity\n",
    "\n",
    "#### What is a phonetic encoding?\n",
    "\n",
    "We can convert a word to a representation of its pronunciation. Ofcourse, this might vary by accents, and the conversion technique as well. \n",
    "\n",
    "Yet, over time 2-3 popular ways have emerged to do this. Each of these methods takes a single string and returns a coded representation. I encourage you to google each of these terms\n",
    "\n",
    "- American Soundex (1930s) - implemented in popular database software such as PostgreSQL, MySQL, SQLite\n",
    "- NYSIIS (New York State Identification and Intelligence System) - from 1970s\n",
    "- Metaphone (1990s)\n",
    "- Match Rating Codex (early 2000s)\n",
    "\n",
    "Let's take a quick preview of the same: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'J412'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jellyfish.soundex('Jellyfish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JALYF'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jellyfish.nysiis('Jellyfish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JLFX'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jellyfish.metaphone('Jellyfish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JLYFSH'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jellyfish.match_rating_codex('Jellyfish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use a string comparison utility that we saw earlier to compare two strings, phonetically. \n",
    "\n",
    "#### Metaphone + Levenshtein\n",
    "\n",
    "For instance, `write` and `right` should have zero levenshtein distance because they are pronounced in the same way. Let's try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jellyfish.levenshtein_distance(jellyfish.metaphone('write'), jellyfish.metaphone('right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This worked as expected. Let's encapsulate this into a utility function as we did earlier. We will use two functions params now: `phonetic_func` and `distance_func`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "examples+= [('write', 'right'), ('Mangalore', 'Bangalore'), ('Delhi', 'Dilli')] # adding a few examples to show how cool this is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\tSound\t\tWord\t\t\tSound\t\tPhonetic Distance\n",
      "Narendra Modi\tNRNTR MT  \tNarendra Modi       \tNRNTR MT  \t0         \n",
      "Narendra Modi\tNRNTR MT  \tNarendra D. Modi    \tNRNTR T MT\t2         \n",
      "Narendra Modi\tNRNTR MT  \tNarendar Modi       \tNRNTR MT  \t0         \n",
      "Gujarat   \tKJRT      \tGujrat              \tKJRT      \t0         \n",
      "write     \tRT        \tright               \tRT        \t0         \n",
      "Mangalore \tMNKLR     \tBangalore           \tBNKLR     \t1         \n",
      "Delhi     \tTLH       \tDilli               \tTL        \t1         \n"
     ]
    }
   ],
   "source": [
    "def calculate_phonetic_distance(phonetic_func, distance_func, examples=examples):\n",
    "    print(\"Word\\t\\tSound\\t\\tWord\\t\\t\\tSound\\t\\tPhonetic Distance\")\n",
    "    for ele in examples:\n",
    "        correct, typo = ele[0], ele[1]\n",
    "        phonetic_correct, phonetic_typo = phonetic_func(correct), phonetic_func(typo)\n",
    "        phonetic_distance = distance_func(phonetic_correct, phonetic_typo)\n",
    "        print(f'{correct:<10}\\t{phonetic_correct:<10}\\t{typo:<20}\\t{phonetic_typo:<10}\\t{phonetic_distance:<10}') \n",
    "        \n",
    "calculate_phonetic_distance(phonetic_func=jellyfish.metaphone, distance_func=jellyfish.levenshtein_distance)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that `Delhi` and `Dilli` are separated - which is not nice. On the other hand, `Narendra` and `Narendar` are marked as similar with zero edit distance, which is quite cool. Let's try a different technique and see how it goes:\n",
    "\n",
    "#### American Soundex\n",
    "\n",
    "We notice that the Soundex is aware of common similar sounding words and gives them separate phonetic encoding. This allows us to separate 'right' from 'write'. \n",
    "\n",
    "This will only work on American/English words though. Indian sounds such as `Narendra Modi` and `Narendra D. Modi` are now considered similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\tSound\t\tWord\t\t\tSound\t\tPhonetic Distance\n",
      "Narendra Modi\tN653      \tNarendra Modi       \tN653      \t0         \n",
      "Narendra Modi\tN653      \tNarendra D. Modi    \tN653      \t0         \n",
      "Narendra Modi\tN653      \tNarendar Modi       \tN653      \t0         \n",
      "Gujarat   \tG263      \tGujrat              \tG263      \t0         \n",
      "write     \tW630      \tright               \tR230      \t2         \n",
      "Mangalore \tM524      \tBangalore           \tB524      \t1         \n",
      "Delhi     \tD400      \tDilli               \tD400      \t0         \n"
     ]
    }
   ],
   "source": [
    "calculate_phonetic_distance(phonetic_func=jellyfish.soundex, distance_func=jellyfish.levenshtein_distance)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtime Complexity\n",
    "\n",
    "We now have the ability to find correct spellings of words or mark them as similar. While processing a large corpus, we can extract all unique words and compare each token against every other token. \n",
    "\n",
    "It would take $O(n^2)$ where $n$ = number of unique tokens in a corpus. This might make the process too slow for a large corpus. \n",
    "\n",
    "The alternative is to use a standard dictionary and expand the same for your corpus. If the dictionary has m unique words, this process now will be $O(m * n)$. Assuming $m << n$, this will be much faster than previous approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the Original Corpus with FlashText\n",
    "\n",
    "Using the dictionary approach, we know have a list of changes which we want to make in our corpus. We keep the original so that we can go back and apply different processing pipelines as we would like. We would want to correct these spellings in a duplicate. \n",
    "\n",
    "We are looking for exact word matches i.e. `Javascript` and `Java` are different entitites and each has to be treated as unique. This makes regex a great tool for our use. \n",
    "\n",
    "Our changes are string substitutions e.g. replace `Banglore` with `Bangalore`. \n",
    "\n",
    "_String Substitution_ is a common use case for standardizing multiple occurrences of the same word. For instance, you might want to replace the word with its lemma which we saw above. Or simply, case standardization. Like replacing Python3 with Python. \n",
    "\n",
    "_Search_ is another use case. We want to quickly check if a word exists in the corpus. E.g. we want to find out whether “Python” was mentioned in a document. \n",
    "\n",
    "This makes the native Python string library and regex a natural choice for us. \n",
    "\n",
    "### Why FlashText?\n",
    "\n",
    "Unfortunately, it turns out that Regex is fast enough if the number of keywords to be searched and replaced is in a few 100s. \n",
    "\n",
    "But what about a webscale corpus with millions of documents and few thousand keywords? Regex can take several days to run over such exact searches because of its linear time complexity. How can we improve this? \n",
    "\n",
    "\n",
    "We use FlashText for this very specific use case: \n",
    "\n",
    "- Few million documents with few thousand keywords\n",
    "- Exact keyword matches - either replace or search the presence of those keywords\n",
    "\n",
    "Of course, there are several different solutions possible to this problem. I recommend this for it's simplicity and focus on solving one problem. It does not require us to learn new syntax or setup specific tools such as ElasticSearch. \n",
    "\n",
    "Comparing Flashtext vs Compiled Regex for Search: \n",
    "\n",
    "![Comparing Flashtext vs Compiled Regex for Search]()\n",
    "\n",
    "Comparing FlashText vs Compiled Regex for Substitutions: \n",
    "\n",
    "![Comparing FlashText vs Compiled Regex for Substitutions]()\n",
    "\n",
    "We notice that while the time taken by Regex scales almost linearly, Flashtext is relatively flat. Now we understand, that we need Flashtext for speed and scale. FlashText has seen lot of love from community. Adopters include [NLProc](https://github.com/NIHOPA/NLPre) - the NLP Preprocessing Toolkit from National Institute of Health. \n",
    "\n",
    "Let's get it:\n",
    "\n",
    "**Installation**\n",
    "\n",
    "We will first install pip the package in our conda environment. We will do from our notebook itself: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flashtext in c:\\users\\nirantk\\anaconda3\\envs\\fastai\\lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install flashtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FlashText source code is available on Github and docs are pretty easy to navigate and use. We will only consider two basic examples here. Let's figure out the syntax for finding keywords which exist in a corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NCR', 'Mumbai']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flashtext.keyword import KeywordProcessor\n",
    "keyword_processor = KeywordProcessor()\n",
    "keyword_processor.add_keyword('Delhi', 'NCR') # notice we are adding tuples here\n",
    "keyword_processor.add_keyword('Bombay', 'Mumbai')\n",
    "keywords_found = keyword_processor.extract_keywords('I love the food in Delhi and the people in Bombay')\n",
    "keywords_found\n",
    "# ['NCR', 'Mumbai']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we replace them now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love the food in NCR and the people in Mumbai'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flashtext.keyword import KeywordProcessor\n",
    "keyword_processor = KeywordProcessor()\n",
    "keyword_processor.add_keyword('Delhi', 'NCR')\n",
    "keyword_processor.add_keyword('Bombay', 'Mumbai')\n",
    "replaced_sentence = keyword_processor.replace_keywords('I love the food in Delhi and the people in Bombay')\n",
    "replaced_sentence\n",
    "# 'I love the food in NCR and the people in Mumbai'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations\n",
    "FlashText only supports English for the time being. Regex can search for keywords based special characters like ^,$,*,\\d,. which are not supported in FlashText. So to match partial words like word\\dvec, we would still have to use regex. But it is excellent for extracting complete words like word2vec."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastAI",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
