{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging Linguistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to pick up a simple use case and see how we can solve that. Then, we repeat this again, but on a slighlty different text corpus and so on. \n",
    "\n",
    "This helps us learn build intuition on how to use linguistics in NLP. As mentioned, I am going to use spaCy here, but you are free to use NLTK or equivalent. There are programmatic differences in their API and style, but the underlying theme remains same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach**:\n",
    "This section is dedicated to introducing you to the ideas and tools from several decades of linguistics. The traditional way to introduce this to take an idea, talk about it at length and then put them together all in one piece like magic. \n",
    "\n",
    "Here, I am going to do the other way around. We will solve 2 problems and in the process, we will use look at the tools. Instead of talking to you about a Number 8 spanner, I am giving you a car engine, and the tools and introducing the tools as I use them myself. \n",
    "\n",
    "**Key Idea**: The Natural Language Pipeline\n",
    "\n",
    "Most NLP tasks are solved in a sequential pipeline, with results from one component feeding to the next. \n",
    "\n",
    "There is a wide variety of data structures used to store the pipeline results and intermediate steps. Here, for simplicity, I am going to use only the data structures already in spaCy and native Python one's like lists and dictionaries. \n",
    "\n",
    "\n",
    "**Challenges**:\n",
    "\n",
    "Here, we will tackle the following real-life inspired challenges: \n",
    "- Redacting names from any document e.g. for GDPR compliance\n",
    "- Making quizzes from any text e.g. from a Wikipedia article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can install spaCy via conda or pip. Since I am in a conda environment, I use the conda install from below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -y spacy \n",
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the English language model provided by spaCy. We are doing to use 'en_core_web_lg', the 'lg' at the end stands for large. This means that this is the most comprehensive and best performing model that spaCy releases for general purpose use.\n",
    "\n",
    "You need to only do this once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is an error above, you can use the smaller model as well. \n",
    "\n",
    "Try:\n",
    "- Windows Shell:```python -m spacy download en``` as **Administrator**\n",
    "- Linux Terminal:```sudo python -m spacy download en ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy # for visualization\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.11'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introducing textacy**:\n",
    "\n",
    "Textacy is a very underappreciated set of tools around spaCy. It's tagline is what it exactly does: NLP, before and after spaCy. It implements tools which use spaCy under the hood, ranging from data streaming utilities for production use to higher level text clustering functions. \n",
    "\n",
    "You can install textacy via pip or conda both. On conda, it's available on the 'conda-forge' channel instead of the main 'conda' channel. We mention this by adding a '-c' flag and the channel name after that.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge textacy \n",
    "# !pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redacting Names with Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge: Replace all human names with [REDACTED] in free text**\n",
    "\n",
    "Consider that you are new engineer at European Bank Co. In preparation for GDPR, bank is scrubbing off names of customers from all their old records and specially internal communications like email and memos. They ask you to do this. \n",
    "\n",
    "The first way is to lookup names of your customers and match each of them against all your emails. This can be painfully slow, and error prone. A customer named John D'Souza, you might simply refer to him as DSouza in an email. An exact match for D'Souza will never be scrubbed.\n",
    "\n",
    "\n",
    "Here, we will use an automatic NLP technique to assist us. We will parse all our emails from spaCy and simply replace the person names with the token [REDACTED]. This would be at least 5-10x faster than matching millions of substrings aginast millions of substrings.\n",
    "\n",
    "We will use a small excerpt from a Harry Potter book, talking about flu as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Madam Pomfrey, the nurse, was kept busy by a sudden spate of colds among the staff and students. Her Pepperup potion worked instantly, though it left the drinker smoking at the ears for several hours afterward. Ginny Weasley, who had been looking pale, was bullied into taking some by Percy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the text with spaCy. This runs the entire NLP pipeline.\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'doc' now contains a parsed version of text. We can use it to do anything we want!. \n",
    "For example, this will print out all the named entities that were detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pomfrey (PERSON)\n",
      "Pepperup (ORG)\n",
      "several hours (TIME)\n",
      "Ginny Weasley (PERSON)\n",
      "Percy (PERSON)\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print(f\"{entity.text} ({entity.label_})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spacy object doc has an atrribute 'ents' which stores all detected entities. In order to find this, spaCy has done few things behind the scenes for us e.g.\n",
    "- Sentence Segmentation i.e. break the long text into smaller sentences\n",
    "- Tokenization i.e. break each sentence into individual words or tokens\n",
    "- Removed Stop Words i.e. remove words like _a, an, the, of_\n",
    "- NER i.e. using statistical techniques, find out which entities are there in the text and label them with entity type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Pomfrey, Pepperup, several hours, Ginny Weasley, Percy)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'doc' object has a specific object called 'ents', short for entities which we can use to lookup all entities in our text. Additionally, each entity has as label. \n",
    "\n",
    "Tip: In spaCy, all information is stored by numeric hashing. So, `entity.label` will be a numeric entry like 378, while `entity.label_` will be human readable e.g. PERSON. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(378, 'PERSON')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity.label, entity.label_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In spaCy, all human readable labels etc can also be explained using the simple spacy.explain(label) syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using spaCy's NER, let's write a simple function to replace each PERSON name with [REDACTED]: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redact_names(text):\n",
    "    doc = nlp(text)\n",
    "    redacted_sentence = []\n",
    "    for token in doc:\n",
    "        if token.ent_type_ == \"PERSON\":\n",
    "            redacted_sentence.append(\"[REDACTED]\")\n",
    "        else:\n",
    "            redacted_sentence.append(token.string)\n",
    "    return \"\".join(redacted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes in text as string, parses it in the doc object using the nlp object which we loaded earlier. Then it traverses each token in the document (remember tokenization?). Each token is added to a list. If the token has the entity type of a person, it is replaced with [REDACTED] instead. \n",
    "\n",
    "At the end, we re-construct the original sentenced by converting this list back to a string. \n",
    "\n",
    "As an exercise, try doing above in-place i.e. by editing the original string itself instead of creating a new string.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Madam [REDACTED], the nurse, was kept busy by a sudden spate of colds among the staff and students. Her Pepperup potion worked instantly, though it left the drinker smoking at the ears for several hours afterward. [REDACTED][REDACTED], who had been looking pale, was bullied into taking some by [REDACTED].'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redact_names(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output is still leaky faucet if you are trying to make GDPR compliant edits. By using two [REDACTED] blocks instead of one, we are disclosing the number of words in a name. This can be seriously harmful if we were to use this in some other context, e.g. redacting location or organisation names too. \n",
    "\n",
    "Let's fix this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redact_names(text):\n",
    "    doc = nlp(text)\n",
    "    redacted_sentence = []\n",
    "    for ent in doc.ents:\n",
    "        ent.merge()\n",
    "    for token in doc:\n",
    "        if token.ent_type_ == \"PERSON\":\n",
    "            redacted_sentence.append(\"[REDACTED]\")\n",
    "        else:\n",
    "            redacted_sentence.append(token.string)\n",
    "    return \"\".join(redacted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do this by using merging entities separately from the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Madam [REDACTED], the nurse, was kept busy by a sudden spate of colds among the staff and students. Her Pepperup potion worked instantly, though it left the drinker smoking at the ears for several hours afterward. [REDACTED], who had been looking pale, was bullied into taking some by [REDACTED].'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redact_names(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Types "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy supports the following entity types in the large language model which we loaded in the nlp object:\n",
    "\n",
    "|Type\t|Description\n",
    "|---|---\n",
    "|PERSON\t|People, including fictional.\n",
    "|NORP\t|Nationalities or religious or political groups.\n",
    "|FAC\t|Buildings, airports, highways, bridges, etc.\n",
    "|ORG\t|Companies, agencies, institutions, etc.\n",
    "|GPE\t|Countries, cities, states.\n",
    "|LOC\t|Non-GPE locations, mountain ranges, bodies of water.\n",
    "|PRODUCT\t|Objects, vehicles, foods, etc. (Not services.)\n",
    "|EVENT\t|Named hurricanes, battles, wars, sports events, etc.\n",
    "|WORK_OF_ART\t|Titles of books, songs, etc.\n",
    "|LAW\t|Named documents made into laws.\n",
    "|LANGUAGE\t|Any named language.\n",
    "|DATE\t|Absolute or relative dates or periods.\n",
    "|TIME\t|Times smaller than a day.\n",
    "|PERCENT\t|Percentage, including \"%\".\n",
    "|MONEY\t|Monetary values, including unit.\n",
    "|QUANTITY\t|Measurements, as of weight or distance.\n",
    "|ORDINAL\t|\"first\", \"second\", etc.\n",
    "|CARDINAL\t|Numerals that do not fall under another type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some examples of above in real world sentences, we will also use the `spacy.explain()` on all entities to build a quick mental model of how these things work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_text_entities(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        print(f'{ent}, Label: {ent.label_}, {spacy.explain(ent.label_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla, Label: ORG, Companies, agencies, institutions, etc.\n",
      "20%, Label: PERCENT, Percentage, including \"%\"\n",
      "the months, Label: DATE, Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "explain_text_entities('Tesla has gained 20% market share in the months since')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taj Mahal, Label: PERSON, People, including fictional\n",
      "Mughal, Label: NORP, Nationalities or religious or political groups\n",
      "Shah Jahan, Label: PERSON, People, including fictional\n",
      "Yamuna, Label: LOC, Non-GPE locations, mountain ranges, bodies of water\n",
      "Agra, Label: GPE, Countries, cities, states\n",
      "India, Label: GPE, Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "explain_text_entities('Taj Mahal built by Mughal Emperor Shah Jahan stands tall on the banks of Yamuna in modern day Agra, India')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, the model got \"Taj Mahal\" wrong. Taj Mahal is obviously a world famous monument. But the model has made a believable mistake, because \"Taj Mahal\" was also the stage name of a Blues musician. \n",
    "\n",
    "In most production use cases, we \"fine-tune\" the in-built spaCy models for specific languages using our own annotations. This would teach the model that Taj Mahal for us is almost always a monument and not a Blues musician. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ashoka, Label: PERSON, People, including fictional\n",
      "Indian, Label: NORP, Nationalities or religious or political groups\n"
     ]
    }
   ],
   "source": [
    "explain_text_entities('Ashoka was a great Indian king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ashoka University, Label: ORG, Companies, agencies, institutions, etc.\n",
      "the Young India Fellowship, Label: ORG, Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "explain_text_entities('The Ashoka University sponsors the Young India Fellowship')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, our pipeline is able to leverage the word 'University' to infer that Ashoka is a name of an organisation and not King Ashoka from Indian history. \n",
    "\n",
    "It has also figured out that 'Young India Fellowship' is one logical entity and has not tagged 'India' has a location. \n",
    "\n",
    "It helps me a lot to see a few examples such as above to form a mental model of what are the limits of what we can and cannot do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Question Generation\n",
    "\n",
    "The Challenge: Can you automatically convert a sentence to a question? \n",
    "\n",
    "For instance, 'Martin Luther King Jr. was a civil rights activist and skilled orator.' to 'Who was Martin Luther King Jr.?' \n",
    "\n",
    "Notice that when we convert a sentence to a question, the answer might not be in the original sentence anymore. To me, the answer to that question might be something different and that's fine. We are not aiming for answers here.\n",
    "\n",
    "### Part-of-Speech Tagging\n",
    "\n",
    "Sometimes, we want to quickly pull out keywords, or keyphrases from a larger body of text. This helps us mentally paint a picture of what this text is about. This is particularly helpful in analysis of texts like long emails or essays. \n",
    "\n",
    "As a quick hack, we can pull out all relevant \"nouns\". This is because most keywords are in fact nouns of some form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = 'Bansoori is an Indian classical instrument. Tom plays Bansoori and Guitar.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need noun chunks. Noun chunks are _noun phrases_ - not a single word, but a short phrase which describes the noun. For example, \"the blue skies\" or \"the worldâ€™s largest conglomerate\". \n",
    "\n",
    "To get the noun chunks in a document, simply iterate over `doc.noun_chunks`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1 Bansoori\n",
      "sentence1 an Indian classical instrument\n",
      "sentence2 Tom\n",
      "sentence2 Bansoori\n",
      "sentence2 Guitar\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(doc.sents):\n",
    "    for noun in sentence.noun_chunks:\n",
    "        print(f'sentence{idx+1}', noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our example text has two sentences, we can pull out noun phrase chunks from each sentence. We pull out noun phrases instead of single words. This means, we are able to pull out 'an Indian classical instrument' as one noun. This is quite useful as we will see in a moment.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take a quick look at all parts-of-speech tags in our example text. We will use the verbs and adjectives to write some simple question generating logic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bansoori PROPN NNP\n",
      "is VERB VBZ\n",
      "an DET DT\n",
      "Indian ADJ JJ\n",
      "classical ADJ JJ\n",
      "instrument NOUN NN\n",
      ". PUNCT .\n",
      "Tom PROPN NNP\n",
      "plays VERB VBZ\n",
      "Bansoori PROPN NNP\n",
      "and CCONJ CC\n",
      "Guitar PROPN NNP\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that here 'instrument' is tagged as a NOUN while 'Indian' and 'classical' are tagged as adjectives. This makes sense. Addititionally, Bansoori and Guitar are tagged as PROPN or Proper Nouns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nouns vs Proper Noun** \n",
    "Nouns name people, places, and things. Common nouns name general items like waiter, jeans, country. Proper nouns name specific things like Roger, Levi's, India"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Ruleset\n",
    "\n",
    "Quite often when using linguistics, you will be writing custom rules. Here is one data structure suggestion to help you store these rules: list of dictionaries. Each dictionary in turn can have elements ranging from simple string lists to lists to strings. Avoid nesting a list of dictionaries inside a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruleset = [\n",
    "    {\n",
    "        'id': 1, \n",
    "        'req_tags': ['NNP', 'VBZ', 'NN'],\n",
    "    }, \n",
    "    {\n",
    "        'id': 2, \n",
    "        'req_tags': ['NNP', 'VBZ'],\n",
    "    }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I have written two rules. Each rule is simply a collection of part-of-speech tags stored under the 'req_tags' key. Each rule comprises of all the tags that I will look for in a particular sentence. \n",
    "\n",
    "Depending on 'id', I will use a hard coded question template to generate my questions. In practice, you can and should move the question template to your ruleset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 1, 'req_tags': ['NNP', 'VBZ', 'NN']}, {'id': 2, 'req_tags': ['NNP', 'VBZ']}]\n"
     ]
    }
   ],
   "source": [
    "print(ruleset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I need a function to pull out all tokens which match a particular tag. We do this by simply iterating over the entire list of and matching each token against the target tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(doc, tag):\n",
    "    return [tok for tok in doc if tok.tag_ == tag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: This is slow O(n). As an exercise, can you think of a way to reduce this to O(1)? \n",
    "\n",
    "Hint: You can pre-compute some results and store them at cost of more memory.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I am going to write a function to use the ruleset above and use a question template. \n",
    "\n",
    "Here is the broad outline which I will follow for each sentence: \n",
    "\n",
    "- For each rule id, check if all the required tags ('req_tags') meet the conditions \n",
    "- Find the first rule id which matches, \n",
    "- Find the words which match the required part of speech tags\n",
    "- Fill in the corresponding question template and return the question string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_ques(sent:str)->str:\n",
    "    \"\"\"\n",
    "    Return a question string corresponding to a sentence string using a set of pre-written rules\n",
    "    \"\"\"\n",
    "    doc = nlp(sent)\n",
    "    pos_tags = [token.tag_ for token in doc]\n",
    "    for idx, rule in enumerate(ruleset):\n",
    "        if rule['id'] == 1:\n",
    "            if all(key in pos_tags for key in rule['req_tags']): \n",
    "                print(f\"Rule id {rule['id']} matched for sentence: {sent}\")\n",
    "                NNP = get_pos_tag(doc, \"NNP\")\n",
    "                NNP = str(NNP[0])\n",
    "                VBZ = get_pos_tag(doc, \"VBZ\")\n",
    "                VBZ = str(VBZ[0])\n",
    "                ques = f'What {VBZ} {NNP}?'\n",
    "                return(ques)\n",
    "        if rule['id'] == 2:\n",
    "            if all(key in pos_tags for key in rule['req_tags']): #'NNP', 'VBZ' in sentence.\n",
    "                print(f\"Rule id {rule['id']} matched for sentence: {sent}\")\n",
    "                NNP = get_pos_tag(doc, \"NNP\")\n",
    "                NNP = str(NNP[0])\n",
    "                VBZ = get_pos_tag(doc, \"VBZ\")\n",
    "                VBZ = str(VBZ[0].lemma_)\n",
    "                ques = f'What does {NNP} {VBZ}?'\n",
    "                return(ques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within each rule id match, I do something more: I am dropping all but the first match for each part-of-speech tag that I receive. For instance, when I query for \"NNP\", I later pick the first element with NNP[0], convert it to string and drop all other matches. \n",
    "\n",
    "While this is a perfectly good approach for simple sentences, this breaks down when you have conditional statements or complex reasoning. Let's run the above function for each sentence in the our example text and see what questions do we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule id 1 matched for sentence: Bansoori is an Indian classical instrument.\n",
      "The generated question is: What is Bansoori?\n",
      "Rule id 2 matched for sentence: Tom plays Bansoori and Guitar.\n",
      "The generated question is: What does Tom play?\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(f\"The generated question is: {sent_to_ques(str(sent))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite good. Obviously, I cheated a bit by writing exactly two rules for the two example sentences I already knew. In practice, you will need a much larger set, maybe 10-15 rulesets and corresponding templates just to have a decent coverage of \"What?\" questions. \n",
    "\n",
    "Another few rulesets might be needed to cover \"When\",\"Who\" and \"Where\" type of questions. For instance, \"Who plays Bansoori?\" is also a valid question from the second sentence above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means PoS tagging+rule driven engine can have a large coverage, a reasonable precision with respect to the questions - but it will still be a little tedious to maintain, debug and generalize this system. \n",
    "\n",
    "We need a set of better tools which is less reliant on the \"state\" of tokens and more on the relationship between them. This will allow us to change the relationship to form a question instead. This is where Dependency Parsing comes in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Generation using Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a dependency parser? \n",
    "\n",
    "> A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between \"head\" words and words which modify those heads.\n",
    "> from [Stanford NNDEP Project](https://nlp.stanford.edu/software/nndep.html)\n",
    "\n",
    "A dependency parser helps us understand the various ways in which parts of the sentence interact or depend on each other. For instance, how is the noun modified by adjectives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bansoori nsubj\n",
      "is ROOT\n",
      "an det\n",
      "Indian amod\n",
      "classical amod\n",
      "instrument attr\n",
      ". punct\n",
      "Tom nsubj\n",
      "plays ROOT\n",
      "Bansoori dobj\n",
      "and cc\n",
      "Guitar conj\n",
      ". punct\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these terms are simple enough to guess e.g. 'ROOT' is where the dependency tree might begin. 'nsubj' is the noun or nominal subject. 'cc' is probably conjunction. But this is still incomplete, luckily for us, spaCy includes the nifty `explain()` function to help us interpret these.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bansoori nsubj nominal subject\n",
      "is ROOT None\n",
      "an det determiner\n",
      "Indian amod adjectival modifier\n",
      "classical amod adjectival modifier\n",
      "instrument attr attribute\n",
      ". punct punctuation\n",
      "Tom nsubj nominal subject\n",
      "plays ROOT None\n",
      "Bansoori dobj direct object\n",
      "and cc coordinating conjunction\n",
      "Guitar conj conjunct\n",
      ". punct punctuation\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.dep_, spacy.explain(token.dep_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a good starting point to Google away and pick up some linguistics specific terms. E.g. a 'conjunct' is often used to connect two clauses. 'attribute' is simply a way to highlight something which is a property of the nominal subject. \n",
    "\n",
    "Nominal subjects are usually nouns or pronouns which in turns are actors (via verbs) or have properties(via attributes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Relationship\n",
    "\n",
    "spaCy has an inbuilt tool called displacy for displaying simple, but clean and powerful visualizations. It offers two primary modes: Named Entity Recognition and Dependency Parsing. Here we will use the 'dep' or dependency mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1975\" height=\"487.0\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial\"><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Bansoori</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">an</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Indian</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">classical</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">instrument.</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Tom</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">plays</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">VERB</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">Bansoori</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">PROPN</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">and</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">CCONJ</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">Guitar.</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">PROPN</tspan></text><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath></text><path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M420,352.0 C420,89.5 920.0,89.5 920.0,352.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath></text><path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M595,352.0 C595,177.0 915.0,177.0 915.0,352.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath></text><path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath></text><path class=\"displacy-arrowhead\" d=\"M770,354.0 L762,342.0 778,342.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M245,352.0 C245,2.0 925.0,2.0 925.0,352.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath></text><path class=\"displacy-arrowhead\" d=\"M925.0,354.0 L933.0,342.0 917.0,342.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath></text><path class=\"displacy-arrowhead\" d=\"M1120,354.0 L1112,342.0 1128,342.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath></text><path class=\"displacy-arrowhead\" d=\"M1435.0,354.0 L1443.0,342.0 1427.0,342.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,264.5 1610.0,264.5 1610.0,352.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath></text><path class=\"displacy-arrowhead\" d=\"M1610.0,354.0 L1618.0,342.0 1602.0,342.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-0-8\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,177.0 1790.0,177.0 1790.0,352.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath></text><path class=\"displacy-arrowhead\" d=\"M1790.0,354.0 L1798.0,342.0 1782.0,342.0\" fill=\"currentColor\"/></g></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first sentence for a quick study: We see that \"instrument\" is \"amod\" or adjectively modified by \"Indian classicial\". We pulled this phrase earlier as a noun chunk. \n",
    "\n",
    "This means that when we pulled noun phrase chunks out of this sentence, spaCy must have finished dependency parsing already under the hood. \n",
    "\n",
    "Also notice the direction of arrows, while the NOUN (instrument) is modified by ADJ. It is the 'attr' of the ROOT VERB (is). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tricky_doc = nlp('This is ship-shipping ship, shipping shipping ships')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1450\" height=\"487.0\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">This</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">ship-</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">shipping</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">ship,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">shipping</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">shipping</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">ships</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M245,352.0 C245,89.5 745.0,89.5 745.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,354.0 L753.0,342.0 737.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M245,352.0 C245,2.0 925.0,2.0 925.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,354.0 L933.0,342.0 917.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,354.0 L1112,342.0 1128,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M945,352.0 C945,177.0 1265.0,177.0 1265.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,354.0 L1273.0,342.0 1257.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(tricky_doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This logical tree structure of simple sentences is what we will exploit to simplify our question generation. In order to do this, we need two important pieces: \n",
    "- the main verb aka ROOT\n",
    "- the subjects on which this ROOT verb is acting\n",
    "\n",
    "Let's write some functions to extract these dependency entities in the spaCy token format i.e. without converting them to strings. \n",
    "\n",
    "Alternatively, we can import them from textacy itself :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy.spacier import utils as spacy_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIP: You can see the docstring AND function implementation using the '??' syntax in Jupyter notebook like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "??spacy_utils.get_main_verbs_of_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signature: spacy_utils.get_main_verbs_of_sent(sent)\n",
    "# Source:   \n",
    "# def get_main_verbs_of_sent(sent):\n",
    "#     \"\"\"Return the main (non-auxiliary) verbs in a sentence.\"\"\"\n",
    "#     return [tok for tok in sent\n",
    "#             if tok.pos == VERB and tok.dep_ not in constants.AUX_DEPS]\n",
    "# File:      d:\\miniconda3\\envs\\nlp\\lib\\site-packages\\textacy\\spacier\\utils.py\n",
    "# Type:      function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are to ask questions from someone, they often are around a piece of information e.g. What is the capital of India? or around some action e.g. What did you do on Sunday?\n",
    "\n",
    "Answering 'what' means we need to find out what the verbs are acting on. This means find the subjects of the verb. Let's take a more concrete but simple example to explore this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_sentence = 'Shivangi is an engineer'\n",
    "doc = nlp(toy_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the entities in this sentence? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Shivangi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " is an engineer</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out the main verb in this sentence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[is]\n"
     ]
    }
   ],
   "source": [
    "verbs = spacy_utils.get_main_verbs_of_sent(doc)\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what are nominal subjects of this verb?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is [Shivangi]\n"
     ]
    }
   ],
   "source": [
    "for verb in verbs:\n",
    "    print(verb, spacy_utils.get_subjects_of_verb(verb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that this has a reasonable overlap with the noun phrases which we pulled from our part-of-speech tagging but can be different as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Shivangi, 'NNP'), (is, 'VBZ'), (an, 'DT'), (engineer, 'NN')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token, token.tag_) for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: As an exercise, extend this approach to at least add Who, Where and When questions as practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level Up: Question and Answer\n",
    "So far, we have been trying to generate questions. But if you were trying to make an automated quiz for students, you would also need to mine the right answer. \n",
    "\n",
    "The answer in this case will be simply the objects of verb. What is an object of verb? \n",
    "\n",
    "> In the sentence, \"Give the book to me,\" \"book\" is the direct object of the verb \"give,\" and \"me\" is the indirect object. - from the Cambridge English Dictionary\n",
    "\n",
    "Loosely, object is the piece on which our verb acts. This is almost always the answer to our \"what\". Let's write a question to find the objects of any verb --- or wait, we can pull it from the `textacy.spacier.utils`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[engineer]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_utils.get_objects_of_verb(verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is [engineer]\n"
     ]
    }
   ],
   "source": [
    "for verb in verbs:\n",
    "    print(verb, spacy_utils.get_objects_of_verb(verb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"750\" height=\"312.0\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial\"><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Shivangi</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">an</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">engineer</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan></text><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath></text><path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath></text><path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath></text><path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/></g></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the output of our functions for the example text. The first is the sentence itself, then the root verb, than the lemma form of that verb, followed by subjects of the verb and then objects.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bansoori is an Indian classical instrument. is be [Bansoori] [instrument]\n",
      "Tom plays Bansoori and Guitar. plays play [Tom] [Bansoori, Guitar]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(example_text)\n",
    "for sentence in doc.sents:\n",
    "    print(sentence, sentence.root, sentence.root.lemma_, spacy_utils.get_subjects_of_verb(sentence.root), spacy_utils.get_objects_of_verb(sentence.root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's arrange the pieces above into a neat function which we can then re-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def para_to_ques(eg_text):\n",
    "    doc = nlp(eg_text)\n",
    "    results = []\n",
    "    for sentence in doc.sents:\n",
    "        root = sentence.root\n",
    "        ask_about = spacy_utils.get_subjects_of_verb(root)\n",
    "        answers = spacy_utils.get_objects_of_verb(root)\n",
    "        if len(ask_about) > 0 and len(answers) > 0:\n",
    "            if root.lemma_ == \"be\":\n",
    "                question = f'What {root} {ask_about[0]}?'\n",
    "            else:\n",
    "                question = f'What does {ask_about[0]} {root.lemma_}?'\n",
    "            results.append({'question':question, 'answers':answers})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is Bansoori?', 'answers': [instrument]},\n",
       " {'question': 'What does Tom play?', 'answers': [Bansoori, Guitar]}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_to_ques(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems right to me. Let's run this on a larger sample of sentences. This sample has varying degrees of complexities and sentence structures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_example_text = \"\"\"\n",
    "Puliyogare is a South Indian dish made of rice and tamarind. \n",
    "Priya writes poems. Shivangi bakes cakes. Sachin sings in the orchestra.\n",
    "\n",
    "Osmosis is the movement of a solvent across a semipermeable membrane toward a higher concentration of solute. In biological systems, the solvent is typically water, but osmosis can occur in other liquids, supercritical liquids, and even gases.\n",
    "When a cell is submerged in water, the water molecules pass through the cell membrane from an area of low solute concentration to high solute concentration. For example, if the cell is submerged in saltwater, water molecules move out of the cell. If a cell is submerged in freshwater, water molecules move into the cell.\n",
    "\n",
    "Raja-Yoga is divided into eight steps. The first is Yama. Yama is nonviolence, truthfulness, continence, and non-receiving of any gifts.\n",
    "After Yama, Raja-Yoga has Niyama. cleanliness, contentment, austerity, study, and self - surrender to God.\n",
    "The steps are Yama and Niyama. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is Puliyogare?', 'answers': [dish]},\n",
       " {'question': 'What does Priya write?', 'answers': [poems]},\n",
       " {'question': 'What does Shivangi bake?', 'answers': [cakes]},\n",
       " {'question': 'What is Osmosis?', 'answers': [movement]},\n",
       " {'question': 'What is solvent?', 'answers': [water]},\n",
       " {'question': 'What is first?', 'answers': [Yama]},\n",
       " {'question': 'What is Yama?',\n",
       "  'answers': [nonviolence, truthfulness, continence, of]},\n",
       " {'question': 'What does Yoga have?', 'answers': [Niyama]},\n",
       " {'question': 'What are steps?', 'answers': [Yama, Niyama]}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_to_ques(large_example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facts Extraction using Semi Structured Sentence Parsing\n",
    "Introducing textacy,\n",
    "\n",
    "Boss mode with co reference resolution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
