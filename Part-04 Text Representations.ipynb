{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Representations: Words to Numbers\n",
    "---\n",
    "\n",
    "Computers today can not act on words or text directly. They need to be represented by meaningful number sequences. These long sequences of decimal numbers are called vectors. \n",
    "\n",
    "Where are these word vectors used?\n",
    "\n",
    "- Text Classification and Summarization tasks\n",
    "- Similar words search e.g. synonyms, logically similar\n",
    "- Machine Translation (e.g Translate text from English to German)\n",
    "- Understanding Similar texts (e.g. fb feed articles) \n",
    "- Question Answering and doing tasks (e.g chatbots in scheduling appointments etc.)\n",
    "\n",
    "Usage\n",
    "---\n",
    "\n",
    "Using a machine learning or deep learning model for classification, with following text vectorization methods: \n",
    "- One Hot embedding\n",
    "- TF-IDF\n",
    "- word2vec by Google\n",
    "- GLove by Stanford\n",
    "- fastText by Facebook\n",
    "\n",
    "Sentence and Document Embeddings \n",
    "---\n",
    "\n",
    "Lastly, we look at text sequences larger than words and try to make sentence and document embeddings. doc2vec is a popular adaptation of the same. We will use gensim and gensim-data to play and evaluate above. \n",
    "\n",
    "Checklist\n",
    "---\n",
    "\n",
    "Level: ADVANCED \n",
    "\n",
    "- Introducing gensim and gensim-data\n",
    "- word2vec, GloVe and modern: ConceptNet-Numberbatch and fastText\n",
    "- Understanding Word Vectors\n",
    "- Integrating with Text Classification\n",
    "\n",
    "What will you be able to do by end of it? \n",
    "- SKILL 1: Vectorization of Text\n",
    "- SKILL 2: Using gensim and gensim-data for topic modeling\n",
    "- SKILL 3: Using word2vec, GloVe and fastText \n",
    "- SKILL 4: Integrating Text Representations with Classification and Basic Visualization\n",
    "- SKILL 5: Creating sentence and document vectors for Information Retrieval by using word2vec adaptations: sent2vec and doc2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim: 3.4.0\n"
     ]
    }
   ],
   "source": [
    "print(f'gensim: {gensim.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download some pre-trained GLove embeddings: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "class TqdmUpTo(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None: self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "def get_data(url, filename):\n",
    "    \"\"\"\n",
    "    Download data if the filename does not exist already\n",
    "    Uses Tqdm to show download progress\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from urllib.request import urlretrieve\n",
    "    \n",
    "    if not os.path.exists(filename):\n",
    "\n",
    "        dirname = os.path.dirname(filename)\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "\n",
    "        with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:\n",
    "            urlretrieve(url, filename, reporthook=t.update_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_url = 'http://nlp.stanford.edu/data/glove.6B.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(embedding_url, 'data/glove.6B.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLoVe or  word2vec?\n",
    "\n",
    "In general, I recommend using **GLoVe over word2vec**. This is because it outperforms word2vec on most machine learning and NLP challenges in academia as well as my limited experience. \n",
    "\n",
    "I am convinced enough to skip the original word2vec completely here. But for the sake of completeness, we will see the following: \n",
    "\n",
    "- How to use the original embeddings? Example: GLoVe\n",
    "- How to handle Out of Vocabulary words? Hint: FastText\n",
    "- How to train your own word2vec vectors on your own corpus? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to run this only once, can unzip manually unzip to the data directory too\n",
    "# !unzip data/glove.6B.zip\n",
    "# !mv glove.6B.300d.txt data/glove.6B.300d.txt \n",
    "# !mv glove.6B.200d.txt data/glove.6B.200d.txt \n",
    "# !mv glove.6B.100d.txt data/glove.6B.100d.txt \n",
    "# !mv glove.6B.50d.txt data/glove.6B.50d.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use pre-trained embeddings?\n",
    "\n",
    "**Challenge**: The file formats used by word2vec and GloVe are slightly different from each other. We'd like a consistent API to lookup any word embedding. We can do this \n",
    "\n",
    "**Solution**: This format conversion can be done using `gensim`'s API called `glove2word2vec`. We will use this to convert our glove embedding information to word2vec format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'data/glove.6B.300d.txt'\n",
    "word2vec_output_file = 'data/glove.6B.300d.word2vec.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(word2vec_output_file):\n",
    "    glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KeyedVectors API\n",
    "We now have the simple task of loading the vectors from a file. We do this using `KeyedVectors` API in gensim. The word we want to lookup is the _key_ and the numerical representation of that word is the corresponding _value_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 10.5 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from gensim.models import KeyedVectors\n",
    "filename = word2vec_output_file \n",
    "# load the Stanford GloVe model\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "# binary=False format for human readable text (.txt) files, and binary=True for .bin files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.6713277101516724)]\n"
     ]
    }
   ],
   "source": [
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('indian', 0.7355823516845703),\n",
       " ('pakistan', 0.7285579442977905),\n",
       " ('delhi', 0.6846907138824463),\n",
       " ('bangladesh', 0.6203191876411438),\n",
       " ('lanka', 0.609517514705658),\n",
       " ('sri', 0.6011613607406616),\n",
       " ('kashmir', 0.5746493935585022),\n",
       " ('nepal', 0.5421023368835449),\n",
       " ('pradesh', 0.5405811071395874),\n",
       " ('maharashtra', 0.518537700176239)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('india')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is missing in both word2vec and GloVe? \n",
    "Both glove and word2vec can not handle words or which they did not see during training. These words are called \"out of vocabulary\" or **OOV** in literature. \n",
    "\n",
    "This is evident if you try to lookup nouns which are not frequently used e.g. a name - the model throws `not in vocabulary` error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"word 'nirant' not in vocabulary\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.most_similar('nirant')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to handle OOV words? \n",
    "The author of word2vec (Mikolov et al.) extended it to create fastText at Facebook. They work more on n-grams instead of the entire words. \n",
    "\n",
    "We can create our own fastText embeddings -  which can handle OOV tokens as well\n",
    "\n",
    "### Get the Dataset\n",
    "\n",
    "We download the subtitles of several TED talk from a public dataset. We will train our fastText embeddings on these as well as the word2vec embeddings for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_dataset = \"https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip\"\n",
    "get_data(ted_dataset, \"data/ted_en.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import lxml.etree\n",
    "# extract subtitle\n",
    "with zipfile.ZipFile('data/ted_en.zip', 'r') as z:\n",
    "    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
    "input_text = '\\n'.join(doc.xpath('//content/text()'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\\nTo me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation. Both are necessary, but it can be too much of a good thing.\\nConsider Facit. I'm actually old enough to remember them. Facit was a fantastic company. They were born deep in the Swedish forest, and they made the best mechanical calculators in the world. Everybody used them. A\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using subtitles from TED talks, there are some fillers which are not useful. These are often words describing sound in the parenthesis and the speaker’s name. \n",
    "\n",
    "Let's remove these fillers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# remove parenthesis \n",
    "input_text_noparens = re.sub(r'\\([^)]*\\)', '', input_text)\n",
    "\n",
    "# store as list of sentences\n",
    "sentences_strings_ted = []\n",
    "for line in input_text_noparens.split('\\n'):\n",
    "    m = re.match(r'^(?:(?P<precolon>[^:]{,20}):)?(?P<postcolon>.*)$', line)\n",
    "    sentences_strings_ted.extend(sent for sent in m.groupdict()['postcolon'].split('.') if sent)\n",
    "\n",
    "# store as list of lists of words\n",
    "sentences_ted = []\n",
    "for sent_str in sentences_strings_ted:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", sent_str.lower()).split()\n",
    "    sentences_ted.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise for the reader: \n",
    "    Replace the .split() used above with the tokenizer from spacy and see how the `senetences_ted` changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_ted[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each `sentenced_ted` is now a list of list. Each element of the first list is a sentence, and each sentence is a list of tokens (e.g. words).\n",
    "\n",
    "This is the expected structure for training text embeddings using `gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train FastText Embedddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.87 s, sys: 444 ms, total: 9.31 s\n",
      "Wall time: 9.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_ted = FastText(sentences_ted, size=100, window=5, min_count=5, workers=-1, sg=1)\n",
    "# sg = 1 denotes skipgram, else CBOW is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('indians', 0.5911639928817749),\n",
       " ('indian', 0.5406097769737244),\n",
       " ('indiana', 0.4898717999458313),\n",
       " ('indicated', 0.4400438070297241),\n",
       " ('indicate', 0.4042605757713318),\n",
       " ('internal', 0.39166826009750366),\n",
       " ('interior', 0.3871103823184967),\n",
       " ('byproducts', 0.3752930164337158),\n",
       " ('princesses', 0.37265270948410034),\n",
       " ('indications', 0.369659960269928)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ted.wv.most_similar(\"india\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train word2vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.6 s, sys: 80 ms, total: 2.68 s\n",
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_word2vec_ted = Word2Vec(sentences=sentences_ted, size=100, window=5, min_count=5, workers=-1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('174', 0.38140085339546204),\n",
       " ('specifies', 0.3672548234462738),\n",
       " ('praising', 0.36509859561920166),\n",
       " ('offshore', 0.3498424291610718),\n",
       " ('objection', 0.34258580207824707),\n",
       " ('h', 0.34086084365844727),\n",
       " ('slapped', 0.3404659032821655),\n",
       " ('iconography', 0.3358972668647766),\n",
       " ('paintbrush', 0.33297550678253174),\n",
       " ('sprinter', 0.33028823137283325)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_word2vec_ted.wv.most_similar(\"india\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText or word2vec? \n",
    "\n",
    "According to the preliminary comparisons by gensim: \n",
    "> fastText embeddings are significantly better than word2vec at encoding syntactic information. This is expected, since most syntactic analogies are morphology based, and the char n-gram approach of fastText takes such information into account. The original word2vec model seems to perform better on semantic tasks, since words in semantic analogies are unrelated to their char n-grams, and the added information from irrelevant char n-grams worsens the embeddings.\n",
    "\n",
    "> Source: [word2vec fasttext comparison notebook](https://github.com/RaRe-Technologies/gensim/blob/37e49971efa74310b300468a5b3cf531319c6536/docs/notebooks/Word2Vec_FastText_Comparison.ipynb)\n",
    "\n",
    "In general, prefer fasttext for most web-scale systems because of it's capability to handle words which it has not seen in training. It is definitely better than word2vec on small data, and at least as good as word2vec on larger datasets. \n",
    "\n",
    "Here is a thumb rule for commercial applications: fastText > GloVe > word2vec. This is obviously not always true, but empirically most common result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Embeddings\n",
    "This section is based on the [Doc2Vec API Tutorial](https://github.com/RaRe-Technologies/gensim/blob/37e49971efa74310b300468a5b3cf531319c6536/docs/notebooks/doc2vec-wikipedia.ipynb) from gensim repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we already have the wikicorpus data format implemented in gensim. Additonally, `doc2vec` expects data loaders in a particular format: as objects of the `TaggedDocument` class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the corpus\n",
    "\n",
    "1. Download the latest Wikipedia dump (filename: enwiki-latest-pages-articles.xml.bz2) from [Wiki Dumps here](https://dumps.wikimedia.org/enwiki/latest/)\n",
    "1. Convert the articles to `WikiCorpus`. WikiCorpus construct a corpus from a Wikipedia (or other MediaWiki-based) database dump. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "enwiki-latest-pages-articles.xml.bz2:   5%|▍         | 699M/15.2G [06:06<1:55:38, 2.09MB/s]   "
     ]
    }
   ],
   "source": [
    "# This downloads a large (greater than 15G file, do not download multiple copies)\n",
    "get_data('https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2', \"data/enwiki-latest-pages-articles.xml.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = WikiCorpus(\"data/enwiki-latest-pages-articles.xml.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggedWikiDocument(object):\n",
    "    def __init__(self, wiki):\n",
    "        self.wiki = wiki\n",
    "        self.wiki.metadata = True\n",
    "    def __iter__(self):\n",
    "        for content, (page_id, title) in self.wiki.get_texts():\n",
    "            yield TaggedDocument([c.decode(\"utf-8\") for c in content], [title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = TaggedWikiDocument(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(documents)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = Doc2Vec(min_count=0)\n",
    "pre.scan_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
