{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Representations: Words to Numbers\n",
    "---\n",
    "\n",
    "Computers today can not act on words or text directly. They need to be represented by meaningful number sequences. These long sequences of decimal numbers are called vectors. \n",
    "\n",
    "Where are these word vectors used?\n",
    "\n",
    "- Text Classification and Summarization tasks\n",
    "- Similar words search e.g. synonyms, logically similar\n",
    "- Machine Translation (e.g Translate text from English to German)\n",
    "- Understanding Similar texts (e.g. fb feed articles) \n",
    "- Question Answering and doing tasks (e.g chatbots in scheduling appointments etc.)\n",
    "\n",
    "Code Examples\n",
    "---\n",
    "\n",
    "Using a machine learning or deep learning model for classification, with following text vectorization methods: \n",
    "- TF-IDF in sklearn pipelines with Logistic Regression\n",
    "- GLove by Stanford, looked up via gensim - we will use pretrained vectors\n",
    "- fastText by Facebook - using pretrained vectors \n",
    "\n",
    "Vectorizing a Specific Dataset\n",
    "- How to train our own word2vec vectors? \n",
    "- How to train our own fastText vectors? \n",
    "- Use the `most_similar` words to compare both of the above\n",
    "\n",
    "Document Embeddings \n",
    "- Document modeling using doc2vec on a small dataset\n",
    "- Using document embedding for clustering\n",
    "\n",
    "Sentence Embeddings\n",
    "- Using Bag of Words of ElMo (figure out how to lookup ElMo embeddings)\n",
    "- Using pretrained InferSent in PyTorch embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nirantk\\Anaconda3\\envs\\fastai\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim: 3.4.0\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(f'gensim: {gensim.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download some pre-trained GLove embeddings: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "class TqdmUpTo(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None: self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "def get_data(url, filename):\n",
    "    \"\"\"\n",
    "    Download data if the filename does not exist already\n",
    "    Uses Tqdm to show download progress\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from urllib.request import urlretrieve\n",
    "    \n",
    "    if not os.path.exists(filename):\n",
    "\n",
    "        dirname = os.path.dirname(filename)\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "\n",
    "        with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:\n",
    "            urlretrieve(url, filename, reporthook=t.update_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "get_data(embedding_url, 'data/glove.6B.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLoVe or  word2vec?\n",
    "\n",
    "\n",
    "In general, I recommend using **GLoVe over word2vec**. This is because it outperforms word2vec on most machine learning and NLP challenges in academia as well as my limited experience. \n",
    "\n",
    "I am convinced enough to skip the original word2vec completely here. But for the sake of completeness, we will see the following: \n",
    "\n",
    "- How to use the original embeddings? Example: GLoVe\n",
    "- How to handle Out of Vocabulary words? Hint: FastText\n",
    "- How to train your own word2vec vectors on your own corpus? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We need to run this only once, can unzip manually unzip to the data directory too\n",
    "# !unzip data/glove.6B.zip \n",
    "# !mv glove.6B.300d.txt data/glove.6B.300d.txt \n",
    "# !mv glove.6B.200d.txt data/glove.6B.200d.txt \n",
    "# !mv glove.6B.100d.txt data/glove.6B.100d.txt \n",
    "# !mv glove.6B.50d.txt data/glove.6B.50d.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use pre-trained embeddings?\n",
    "\n",
    "**Challenge**: The file formats used by word2vec and GloVe are slightly different from each other. We'd like a consistent API to lookup any word embedding. We can do this \n",
    "\n",
    "**Solution**: This format conversion can be done using `gensim`'s API called `glove2word2vec`. We will use this to convert our glove embedding information to word2vec format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'data/glove.6B.300d.txt'\n",
    "word2vec_output_file = 'data/glove.6B.300d.word2vec.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(word2vec_output_file):\n",
    "    glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KeyedVectors API\n",
    "We now have the simple task of loading the vectors from a file. We do this using `KeyedVectors` API in gensim. The word we want to lookup is the _key_ and the numerical representation of that word is the corresponding _value_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = word2vec_output_file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load the Stanford GloVe model from file, this is Disk I/O and can be slow\n",
    "pretrained_w2v_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "# binary=False format for human readable text (.txt) files, and binary=True for .bin files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nirantk\\Anaconda3\\envs\\fastai\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.6713277101516724)]\n"
     ]
    }
   ],
   "source": [
    "# calculate: (king - man) + woman = ?\n",
    "result = pretrained_w2v_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('twitter', 0.37966805696487427)]\n"
     ]
    }
   ],
   "source": [
    "# calculate: (india - canada) +  = ?\n",
    "result = pretrained_w2v_model.most_similar(positive=['quora', 'facebook'], negative=['linkedin'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('indian', 0.7355823516845703),\n",
       " ('pakistan', 0.7285579442977905),\n",
       " ('delhi', 0.6846907138824463),\n",
       " ('bangladesh', 0.6203191876411438),\n",
       " ('lanka', 0.609517514705658),\n",
       " ('sri', 0.6011613607406616),\n",
       " ('kashmir', 0.5746493935585022),\n",
       " ('nepal', 0.5421023368835449),\n",
       " ('pradesh', 0.5405811071395874),\n",
       " ('maharashtra', 0.518537700176239)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_w2v_model.most_similar('india')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is missing in both word2vec and GloVe? \n",
    "Both glove and word2vec can not handle words or which they did not see during training. These words are called \"out of vocabulary\" or **OOV** in literature. \n",
    "\n",
    "This is evident if you try to lookup nouns which are not frequently used e.g. an uncommon person's name - the model throws a `not in vocabulary` error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nirantk\\Anaconda3\\envs\\fastai\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"word 'nirant' not in vocabulary\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pretrained_w2v_model.wv.most_similar('nirant')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to handle OOV words? \n",
    "The author of word2vec (Mikolov et al.) extended it to create fastText at Facebook. They work on character n-grams instead of the entire words. Character n-grams are effective in languages with specific morphological properties s \n",
    "\n",
    "We can create our own fastText embeddings -  which can handle OOV tokens as well\n",
    "\n",
    "### Get the Dataset\n",
    "\n",
    "We download the subtitles of several TED talk from a public dataset. We will train our fastText embeddings on these as well as the word2vec embeddings for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_dataset = \"https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip\"\n",
    "get_data(ted_dataset, \"data/ted_en.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import lxml.etree\n",
    "with zipfile.ZipFile('data/ted_en.zip', 'r') as z:\n",
    "    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
    "input_text = '\\n'.join(doc.xpath('//content/text()'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\\nTo me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation. Both are necessary, but it can be too much of a good thing.\\nConsider Facit. I'm actually old enough to remember them. Facit was a fantastic company. They were born deep in the Swedish forest, and they made the best mechanical calculators in the world. Everybody used them. A\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using subtitles from TED talks, there are some fillers which are not useful. These are often words describing sound in the parenthesis and the speakerâ€™s name. \n",
    "\n",
    "Let's remove these fillers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# remove parenthesis \n",
    "input_text_noparens = re.sub(r'\\([^)]*\\)', '', input_text)\n",
    "\n",
    "# store as list of sentences\n",
    "sentences_strings_ted = []\n",
    "for line in input_text_noparens.split('\\n'):\n",
    "    m = re.match(r'^(?:(?P<precolon>[^:]{,20}):)?(?P<postcolon>.*)$', line)\n",
    "    sentences_strings_ted.extend(sent for sent in m.groupdict()['postcolon'].split('.') if sent)\n",
    "\n",
    "# store as list of lists of words\n",
    "sentences_ted = []\n",
    "for sent_str in sentences_strings_ted:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", sent_str.lower()).split()\n",
    "    sentences_ted.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise for the reader: \n",
    "    Replace the .split() used above with the tokenizer from spacy and see how the `senetences_ted` changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_ted[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each `sentenced_ted` is now a list of list. Each element of the first list is a sentence, and each sentence is a list of tokens (e.g. words).\n",
    "\n",
    "This is the expected structure for training text embeddings using `gensim`. We will write this to disk for easy retrieval later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# with open('ted_clean_sentences.json', 'w') as fp:\n",
    "#     json.dump(sentences_ted, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ted_clean_sentences.json', 'r') as fp:\n",
    "    sentences_ted = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_ted[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train FastText Embedddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fasttext_ted_model = FastText(sentences_ted, size=100, window=5, min_count=5, workers=-1, sg=1)\n",
    "# sg = 1 denotes skipgram, else CBOW is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('indians', 0.5911639928817749),\n",
       " ('indian', 0.5406097769737244),\n",
       " ('indiana', 0.4898717999458313),\n",
       " ('indicated', 0.4400438070297241),\n",
       " ('indicate', 0.4042605757713318),\n",
       " ('internal', 0.39166826009750366),\n",
       " ('interior', 0.3871103823184967),\n",
       " ('byproducts', 0.3752930164337158),\n",
       " ('princesses', 0.37265270948410034),\n",
       " ('indications', 0.369659960269928)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_ted_model.wv.most_similar(\"india\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train word2vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word2vec_ted_model = Word2Vec(sentences=sentences_ted, size=100, window=5, min_count=5, workers=-1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cent', 0.38214215636253357),\n",
       " ('dichotomy', 0.37258434295654297),\n",
       " ('executing', 0.3550642132759094),\n",
       " ('capabilities', 0.3549191951751709),\n",
       " ('enormity', 0.3421599268913269),\n",
       " ('abbott', 0.34020164608955383),\n",
       " ('resented', 0.33033430576324463),\n",
       " ('egypt', 0.32998529076576233),\n",
       " ('reagan', 0.32638251781463623),\n",
       " ('squeezing', 0.32618749141693115)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_ted_model.wv.most_similar(\"india\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText or word2vec? \n",
    "\n",
    "According to the preliminary comparisons by gensim: \n",
    "> fastText embeddings are significantly better than word2vec at encoding syntactic information. This is expected, since most syntactic analogies are morphology based, and the char n-gram approach of fastText takes such information into account. The original word2vec model seems to perform better on semantic tasks, since words in semantic analogies are unrelated to their char n-grams, and the added information from irrelevant char n-grams worsens the embeddings.\n",
    "\n",
    "> Source: [word2vec fasttext comparison notebook](https://github.com/RaRe-Technologies/gensim/blob/37e49971efa74310b300468a5b3cf531319c6536/docs/notebooks/Word2Vec_FastText_Comparison.ipynb)\n",
    "\n",
    "In general, prefer fasttext for most web-scale systems because of it's capability to handle words which it has not seen in training. It is definitely better than word2vec on small data (as evident above), and at least as good as word2vec on larger datasets. \n",
    "\n",
    "Here is a thumb rule for commercial applications: fastText > GloVe > word2vec. This is obviously not always true, but empirically most common result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Embeddings\n",
    "This section is based on the [Doc2Vec API Tutorial](https://github.com/RaRe-Technologies/gensim/blob/37e49971efa74310b300468a5b3cf531319c6536/docs/notebooks/doc2vec-wikipedia.ipynb) from gensim repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from pprint import pprint\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import lxml.etree\n",
    "with zipfile.ZipFile('data/ted_en.zip', 'r') as z:\n",
    "    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
    "talk_titles = doc.xpath('//head//title/text()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "talks = doc.xpath('//content/text()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Knut Haanaes: Two reasons companies fail -- and how to avoid them',\n",
       " \"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\\nTo me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation. Both are necessary, but it can be too much of a good thing.\\nConsider Facit. I'm actually old enough to remember them. Facit was a fantastic company. They were born deep in the Swedish forest, and they made the best mechanical calculators in the world. Everybody used them. A\")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talk_titles[0], talks[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield TaggedDocument(words=doc.split(),tags=[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_talk_docs = LabeledLineSentence(doc_list=talks, labels_list=talk_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "print(cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, epochs=5, workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(ted_talk_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0031104173193676025"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.similarity(22, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = 'Modern medicine has changed the way we think about healthcare, life spans and by extension career and marriage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_2 = 'Modern medicine is not just a boon to the rich, making the raw chemicals behind these is also pollutes the poorest neighborhoods'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_3 = 'Modern medicine has changed the way we think about healthcare, and increased life spans, delaying weddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10078589398909382"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.similarity_unseen_docs(model, sentence_1.split(), sentence_3.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04586911880630877"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.similarity_unseen_docs(model, sentence_1.split(), sentence_2.split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastAI",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
